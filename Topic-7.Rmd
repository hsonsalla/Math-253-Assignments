# Topic 7 Exercises: Nonlinear regression

### Hannah Sonsalla

## Programming Assignment
### 7.9.11

Explore backfitting in context of multiple linear regression.

a) Generate response Y and two predictors X1 and X2, with n = 100.
```{r}
set.seed(1)
X1 = rnorm(100)
X2 = rnorm(100)
Y = 3 + 2 * X1 + 7 * X2 + rnorm(100)
```

b) Initialize beta1 to take on a value of your choice.  It does not matter what value you choose.
```{r}
beta1 = 1.3
```

c) Keeping beta1 fixed, fit the model Y - beta1X1 = beta0 + beta2X2 + e
```{r}
a = Y - beta1 * X1
beta2 = lm(a~X2)$coef[2]
beta2
```

d) Keeping beta2 fixed, fit the model Y - beta2X2 = beta0 + beta1X1 + e
```{r}
a = Y - beta2 * X2
beta1 = lm(a~X1)$coef[2]
beta1
```

e) Write a for loop to repeac (c) and (d) 1000 times.  Report estimates of beta0, beta1, and beta2 at each iteration of the for loop.  Create a plot in which each of these values is displayed, with beta0, beta1 and beta2 each shown in a different color.
```{r}
# Lists for beta0, beta1, beta2
beta0list = rep(0, 1000)
beta1list = rep(0, 1000)
beta2list = rep(0, 1000) 

beta1 = 10

# for loop repeating for 1000 iterations
for (i in 1:1000){
  # beta2
  a = Y - beta1 * X1
  beta2 = lm(a~X2)$coef[2]
  beta2list[i] = beta2
  
  # beta1
  a = Y - beta2 * X2
  beta1 = lm(a~X1)$coef[2]
  beta1list[i] = beta1
  
  #beta0
  a = Y - beta1 * X1 - beta2 * X2
  beta0 = lm(a~1)$coef[1]
  beta0list[i] = beta0
}
```

```{r}
# Plot
plot(1:1000, beta0list, type = "l", xlab = "iteration", ylab = "betas", col = "green", ylim = c(0,10))
lines(1:1000, beta1list, col = "red")
lines(1:1000, beta2list, col = "blue")
legend("topright", c("beta0", "beta1", "beta2"), lty = 1, col = c("green", "red", "blue"))

```

f) Compare answer in (e) to results of simply performing multiple linear regression to predict Y using X1 and X2.  Use abline() to overlay multiple linear regression coefficient estimates on the plot obtained in (e).
```{r}
lm.fit = lm(Y ~ X1 + X2)
plot(1:1000, beta0list, type = "l", xlab = "iteration", ylim = c(0,10), ylab = "betas", col = "green", lwd = 3)
lines(1:1000, beta1list, col = "red", lwd = 3)
lines(1:1000, beta2list, col = "blue", lwd = 3)
abline(h = lm.fit$coef[1], lty = "dashed", lwd = 2)
abline(h = lm.fit$coef[2], lty = "dashed", lwd = 2)
abline(h = lm.fit$coef[3], lty = "dashed", lwd = 2)

legend("topright", c("beta0", "beta1", "beta2", "multiple regression"), lty = 1, col = c("green", "red", "blue", "black"))
```

The dotted lines on the plot show the results of performing multiple lineaar regression.  It appears that the estimated multiple regression coefficients match exactly with the coefficients obtained using backfitting.

g) On this data set, how many backfitting iterations were required in order to obtain a "good" approximation to the multiple regression coefficient estimates?

The relationship between X and Y is linear in this case.  One iteration is enough to obtain a good approximation of true regression coefficients.  This is shown in the plots where the backfitting coefficients immediately match or converge quickly to the multiple regression coefficient estimates.

## Theory Assignment
### 7.9.3

Fitting linear regression model, obtained coefficient estimates of beta 0 = 1, beta 1 = 1, beta 2 = -2.  Sketch estimated curve between X = -2 and X = 2.  Note the intercepts, slopes and other relevant information.

By substituting values of beta and basis functions into the linear regression model, I obtain Y = 1 + 1*X + -2 (X-1)^2 I(X>1).  I plot this with values of X from -2 to 2.
```{r}
X = -2:2
Y = 1 + 1*X + -2 * (X-1)^2 * I(X>=1)
plot(X, Y)
```

As the plot shows, I obtained the following value pairs (X,Y): (-2, -1), (-1, 0), (0, 1), (1,2), (2,1).  The curve appears to be linear between âˆ’2 and 1 with the equation Y = X + 1.  It looks to be quadratic between 1 and 2 with equation Y = 1 + X - 2*(X-1)^2.

### 7.9.4

Fitting linear regression model, obtained coefficient estimates of beta 0 = 1, beta 1 = 1, beta 2 = 3.  Sketch estimated curve between X = -2 and X = 2.  Note the intercepts, slopes and other relevant information.

By substituting values of beta and basis functions into the linear regression model, I obtain Y = 1 + 1 * I(0<=X<=2)-(X-1)I(1<=X<=2) + 3(X-3)I(3<=X<=4) + I(4<X<=5).  I plot this with values of X from -2 to 2.
```{r}
X = -2:2
Y = 1 + 1 * (I(0<=X)&I(X<=2))-(X-1)*(I(1<=X)&I(X<=2)) + 3*(X-3)*(I(3<=X)&I(X<=4)) + (I(4<X)& I(X<=5))
plot(X,Y)
```

As the plot shows, I obtained the following value pairs (X,Y): (-2, 1), (-1, 1), (0,2), (1,2), (2,1).  The curve appears constant from -2 to -1 with the equation Y = 1.  It is linear from -1 to 0 with equation Y = X + 1.  The curve is constant from 0 to 1 with equation Y = 2.  It is linear between 1 and 2 with equation Y = 3-X.


### 7.9.5

Consider two curves, g1 and g2 which are defined in the book.  Curve g1 has a penalty term involving the 3rd derivative and g2 has a penalty term involving the 4th derivative.

a) As lambda goes to infinity, the smoothing spline g2 will have the smaller training RSS. It will be a higher order polynomial because of the order of the derivative penalty term and it will be more flexible.

b) Since I expect g2 to have a smaller training RSS and be more flexible, it may overfit the data.  For these reasons, as lambda goes to infinity, I believe that g1 will have the smaller test RSS.

c)  If lambda is 0, then g1 and g2 will have the same training RSS and same test RSS since the penalty term is eliminated from both.  g1 and g2 will be equal to the loss function which is the same in both cases.
